{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694b53cb-ebcf-4a7d-aacc-e1fbc1811b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install mtcnn\n",
    "import mtcnn\n",
    "from mtcnn.mtcnn import MTCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd25ed7-2e3d-4c34-be03-7faa638113f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "import pickle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a98459a3-0528-4049-a2de-f04356ddb56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_and_resize(image, target_w=224, target_h=224):\n",
    "    '''this function crop & resize images to target size by keeping aspect ratio'''\n",
    "    if image.ndim == 2:\n",
    "        img_h, img_w = image.shape             # for Grayscale will be   img_h, img_w = img.shape\n",
    "    elif image.ndim == 3:\n",
    "        img_h, img_w, channels = image.shape   # for RGB will be   img_h, img_w, channels = img.shape\n",
    "    target_aspect_ratio = target_w/target_h\n",
    "    input_aspect_ratio = img_w/img_h\n",
    "\n",
    "    if input_aspect_ratio > target_aspect_ratio:\n",
    "        resize_w = int(input_aspect_ratio*target_h)\n",
    "        resize_h = target_h\n",
    "        img = cv2.resize(image, (resize_w , resize_h))\n",
    "        crop_left = int((resize_w - target_w)/2)  ## crop left/right equally\n",
    "        crop_right = crop_left + target_w\n",
    "        new_img = img[:, crop_left:crop_right]\n",
    "    if input_aspect_ratio < target_aspect_ratio:\n",
    "        resize_w = target_w\n",
    "        resize_h = int(target_w/input_aspect_ratio)\n",
    "        img = cv2.resize(image, (resize_w , resize_h))\n",
    "        crop_top = int((resize_h - target_h)/4)   ## crop the top by 1/4 and bottom by 3/4 -- can be changed\n",
    "        crop_bottom = crop_top + target_h\n",
    "        new_img = img[crop_top:crop_bottom, :]\n",
    "    if input_aspect_ratio == target_aspect_ratio:\n",
    "        new_img = cv2.resize(image, (target_w, target_h))\n",
    "\n",
    "    return new_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c292c2e-936b-4fd6-926d-df9d76dbfacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = MTCNN()  # creates detector  \n",
    "\n",
    "def extract_face(img, target_size=(224,224)):\n",
    "    '''this functions extract the face from different images by \n",
    "    1) finds the facial bounding box\n",
    "    2) slightly expands top & bottom boundaries to include the whole face\n",
    "    3) crop into a square shape\n",
    "    4) resize to target image size for modelling\n",
    "    5) if the facial bounding box in step 1 is not found, image will be cropped & resized to 224x224 square'''\n",
    "           \n",
    "    # 1. detect faces in an image\n",
    "      \n",
    "    results = detector.detect_faces(img)\n",
    "    if results == []:    # if face is not detected, call function to crop & resize by keeping aspect ratio\n",
    "        new_face = crop_and_resize(img, target_w=224, target_h=224)    \n",
    "    else:\n",
    "        x1, y1, width, height = results[0]['box']\n",
    "        x2, y2 = x1+width, y1+height\n",
    "        face = img[y1:y2, x1:x2]  # this is the face image from the bounding box before expanding bbox\n",
    "\n",
    "        # 2. expand the top & bottom of bounding box by 10 pixels to ensure it captures the whole face\n",
    "        adj_h = 10\n",
    "\n",
    "        #assign value of new y1\n",
    "        if y1-adj_h <10:\n",
    "            new_y1=0\n",
    "        else:\n",
    "            new_y1 = y1-adj_h\n",
    "\n",
    "        #assign value of new y2    \n",
    "        if y1+height+adj_h < img.shape[0]:\n",
    "            new_y2 = y1+height+adj_h\n",
    "        else:\n",
    "            new_y2 = img.shape[0]\n",
    "        new_height = new_y2 - new_y1\n",
    "\n",
    "        # 3. crop the image to a square image by setting the width = new_height and expand the box to new width\n",
    "        adj_w = int((new_height-width)/2)    \n",
    "\n",
    "        #assign value of new x1\n",
    "        if x1-adj_w < 0:\n",
    "            new_x1=0\n",
    "        else:\n",
    "            new_x1 = x1-adj_w\n",
    "\n",
    "        #assign value of new x2\n",
    "        if x2+adj_w > img.shape[1]:\n",
    "            new_x2 = img.shape[1]\n",
    "        else:\n",
    "            new_x2 = x2+adj_w\n",
    "        new_face = img[new_y1:new_y2, new_x1:new_x2]  # face-cropped square image based on original resolution\n",
    "\n",
    "    # 4. resize image to the target pixel size\n",
    "    sqr_img = cv2.resize(new_face, target_size)   \n",
    "    return sqr_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacf0f1e-8f0b-41aa-8249-2d482efc9c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_label_dict = {0: 'Heart', 1: 'Rectangle', 2: 'Oval', 3: 'Round', 4: 'Square', 5: 'Triangle'}\n",
    "\n",
    "def predict_face_shape(img_array):\n",
    "    '''\n",
    "    this function reads a single image in the form of an array, \n",
    "    and process the image then make predictions.\n",
    "    '''\n",
    "    try:\n",
    "        # first extract the face using bounding box\n",
    "        face_img = extract_face(img_array)  # call function to extract face with bounding box\n",
    "        new_img = cv2.cvtColor(face_img,cv2.COLOR_BGR2RGB) # convert to RGB -- use this for display          \n",
    "        # convert the image for modelling\n",
    "        test_img = np.array(new_img, dtype=float)\n",
    "        test_img = test_img/255\n",
    "        test_img = np.array(test_img).reshape(1, 224, 224, 3)  \n",
    "        # make predictions\n",
    "        pred = model.predict(test_img)        \n",
    "        label = np.argmax(pred,axis=1)\n",
    "        shape = y_label_dict[label[0]]\n",
    "        print(f'Your face shape is {shape}')\n",
    "        pred = np.max(pred)\n",
    "        print(f'Probability {np.around(pred*100,2)}')\n",
    "        plt.imshow(new_img)\n",
    "    except Exception as e:\n",
    "        print(f'Oops!  Something went wrong.  Please try again.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef28de9f-f304-4c0b-95a4-cc6421a8b88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "transfer_path = './SavedModels/'\n",
    "transfer_file = transfer_path + 'vgg16-face-2.keras'\n",
    "model = tf.keras.models.load_model(transfer_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4645eb-bff1-425c-ac8b-67b14e3f4630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create path to images in model_testing\n",
    "\n",
    "test_path = './examples/'\n",
    "test_files = os.listdir(test_path)\n",
    "\n",
    "test_img = []\n",
    "\n",
    "for i in test_files:\n",
    "    img = os.path.join(test_path,i)\n",
    "    test_img.append(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbc274c-665a-4316-9b65-4f5b4ac3869c",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_list = []\n",
    "for i, file in enumerate(test_img):\n",
    "  image_list.append(cv2.imread(os.path.join(test_path,file)))\n",
    "len(image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d62ad4-d59e-4bde-9f66-90d4bc1bd544",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.cvtColor(image_list[0],cv2.COLOR_BGR2RGB));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f012e9-bc63-4da5-bea5-8ebc3cca6ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_face_shape(image_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97ba3ce-a415-4dc9-8d19-9237fd97bbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.cvtColor(image_list[1],cv2.COLOR_BGR2RGB));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b329a3-9f44-4bd8-af53-12de8bf26eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_face_shape(image_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7094221c-a624-4ab6-854d-0a95f2f04446",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.cvtColor(image_list[2],cv2.COLOR_BGR2RGB));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5881c247-8370-4748-8b20-a7c306f769f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_face_shape(image_list[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da3bb00-f5c9-4c3f-93d0-2c546812368f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.cvtColor(image_list[3],cv2.COLOR_BGR2RGB));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fb3a02-7c4f-42cc-bec1-11b9b5bb6ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_face_shape(image_list[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c496d9-4208-453a-aa33-c5194c225b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.cvtColor(image_list[4],cv2.COLOR_BGR2RGB));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e38dbba-c2df-457a-99fd-ed4a17c7e625",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_face_shape(image_list[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a91d387-8f7d-4ba8-adb7-49cc8c9ab3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.cvtColor(image_list[5],cv2.COLOR_BGR2RGB));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c3ee63-146c-4acf-93f5-6d65fab1b71e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_face_shape(image_list[5])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
